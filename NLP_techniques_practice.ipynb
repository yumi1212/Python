{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "NLP_techniques_practice.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffe4213e"
      },
      "source": [
        "# NLP AI Applications\n",
        "@Yu-Wei Hsu"
      ],
      "id": "ffe4213e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bbbb1eb"
      },
      "source": [
        "Install NLTK package"
      ],
      "id": "8bbbb1eb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f3e35ac"
      },
      "source": [
        "pip install nltk"
      ],
      "id": "4f3e35ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23c0ff2"
      },
      "source": [
        "## Gutenberg"
      ],
      "id": "e23c0ff2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7c82bb1"
      },
      "source": [
        "Download Gutenberg corpus tool in NLTK package"
      ],
      "id": "b7c82bb1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd2c9197",
        "outputId": "7896c2d8-9fd2-4590-a77b-11b7b821499b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "id": "bd2c9197",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8789ab5"
      },
      "source": [
        " Check each text name in corpus"
      ],
      "id": "e8789ab5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ac1743",
        "outputId": "5baa3188-0012-4aa0-abf1-80883a14b621"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids()"
      ],
      "id": "87ac1743",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5875d67e"
      },
      "source": [
        "Count the relative frequencies of each words in modals for each text"
      ],
      "id": "5875d67e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "f0f3e60d",
        "outputId": "553bc3c0-ed76-4e62-8661-e9e1ac407055"
      },
      "source": [
        "import pandas as pd\n",
        "modals = ['can','could','may','might','will','would','should']\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# for loop for iterating each text \n",
        "for txt in gutenberg.fileids():\n",
        "    # read the words in the text\n",
        "    content = gutenberg.words(txt)\n",
        "    freq = []\n",
        "    \n",
        "    # for loop for counting each modal \n",
        "    for m in modals:\n",
        "        freq.append(content.count(m))\n",
        "\n",
        "    # create relative frequncy list from frequncy list\n",
        "    rel_freq = [round(c/sum(freq),3) for c in freq]\n",
        "    # append in the data frame\n",
        "    df[txt] = rel_freq\n",
        "\n",
        "# swap index and column\n",
        "df = df.T\n",
        "df.columns = modals\n",
        "df"
      ],
      "id": "f0f3e60d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>can</th>\n",
              "      <th>could</th>\n",
              "      <th>may</th>\n",
              "      <th>might</th>\n",
              "      <th>will</th>\n",
              "      <th>would</th>\n",
              "      <th>should</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>austen-emma.txt</th>\n",
              "      <td>0.080</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>austen-persuasion.txt</th>\n",
              "      <td>0.067</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>austen-sense.txt</th>\n",
              "      <td>0.092</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bible-kjv.txt</th>\n",
              "      <td>0.031</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>blake-poems.txt</th>\n",
              "      <td>0.476</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bryant-stories.txt</th>\n",
              "      <td>0.133</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.256</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>burgess-busterbrown.txt</th>\n",
              "      <td>0.130</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carroll-alice.txt</th>\n",
              "      <td>0.197</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chesterton-ball.txt</th>\n",
              "      <td>0.160</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chesterton-brown.txt</th>\n",
              "      <td>0.177</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chesterton-thursday.txt</th>\n",
              "      <td>0.174</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>edgeworth-parents.txt</th>\n",
              "      <td>0.145</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>melville-moby_dick.txt</th>\n",
              "      <td>0.120</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>milton-paradise.txt</th>\n",
              "      <td>0.165</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shakespeare-caesar.txt</th>\n",
              "      <td>0.056</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shakespeare-hamlet.txt</th>\n",
              "      <td>0.085</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.339</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shakespeare-macbeth.txt</th>\n",
              "      <td>0.097</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>whitman-leaves.txt</th>\n",
              "      <td>0.138</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.410</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           can  could    may  might   will  would  should\n",
              "austen-emma.txt          0.080  0.245  0.063  0.096  0.166  0.242   0.109\n",
              "austen-persuasion.txt    0.067  0.297  0.058  0.111  0.108  0.235   0.124\n",
              "austen-sense.txt         0.092  0.253  0.075  0.096  0.158  0.226   0.101\n",
              "bible-kjv.txt            0.031  0.024  0.149  0.069  0.552  0.064   0.111\n",
              "blake-poems.txt          0.476  0.071  0.119  0.048  0.071  0.071   0.143\n",
              "bryant-stories.txt       0.133  0.274  0.032  0.041  0.256  0.196   0.068\n",
              "burgess-busterbrown.txt  0.130  0.316  0.017  0.096  0.107  0.260   0.073\n",
              "carroll-alice.txt        0.197  0.252  0.038  0.097  0.083  0.241   0.093\n",
              "chesterton-ball.txt      0.160  0.143  0.110  0.084  0.242  0.170   0.092\n",
              "chesterton-brown.txt     0.177  0.238  0.066  0.100  0.156  0.185   0.079\n",
              "chesterton-thursday.txt  0.174  0.221  0.083  0.106  0.162  0.173   0.080\n",
              "edgeworth-parents.txt    0.145  0.180  0.068  0.054  0.221  0.215   0.116\n",
              "melville-moby_dick.txt   0.120  0.118  0.126  0.100  0.207  0.230   0.099\n",
              "milton-paradise.txt      0.165  0.096  0.179  0.151  0.248  0.076   0.085\n",
              "shakespeare-caesar.txt   0.056  0.062  0.122  0.042  0.448  0.139   0.132\n",
              "shakespeare-hamlet.txt   0.085  0.067  0.145  0.073  0.339  0.155   0.135\n",
              "shakespeare-macbeth.txt  0.097  0.069  0.139  0.023  0.287  0.194   0.190\n",
              "whitman-leaves.txt       0.138  0.077  0.134  0.041  0.410  0.134   0.066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d415af6"
      },
      "source": [
        "Find two modals with the largest span of relative frequencies, then compare the texts which use the modals most and least."
      ],
      "id": "8d415af6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aee2ae0a",
        "outputId": "15758fdf-7173-43e7-fd6e-bcc8479f2fc6"
      },
      "source": [
        "for m in modals:\n",
        "    print('{}: {}'.format(m,(max(df[m]) - min(df[m]))))"
      ],
      "id": "aee2ae0a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "can: 0.44499999999999995\n",
            "could: 0.292\n",
            "may: 0.16199999999999998\n",
            "might: 0.128\n",
            "will: 0.48100000000000004\n",
            "would: 0.196\n",
            "should: 0.124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27152822"
      },
      "source": [
        "Two modals with the largest span of relative frequencies are 'can' and 'will'. "
      ],
      "id": "27152822"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "284618dd",
        "outputId": "8f759ff7-b600-4f44-b4b4-291326e4e22d"
      },
      "source": [
        "print('The text uses the word \"{}\" most is {}.'.format('can',df['can'].idxmax()))\n",
        "print('The text uses the word \"{}\" least is {}.'.format('can',df['can'].idxmin()))\n",
        "print('The text uses the word \"{}\" most is {}.'.format('will',df['will'].idxmax()))\n",
        "print('The text uses the word \"{}\" least is {}.'.format('will',df['will'].idxmin()))"
      ],
      "id": "284618dd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The text uses the word \"can\" most is blake-poems.txt.\n",
            "The text uses the word \"can\" least is bible-kjv.txt.\n",
            "The text uses the word \"will\" most is bible-kjv.txt.\n",
            "The text uses the word \"will\" least is blake-poems.txt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "c221990d",
        "outputId": "7577ac78-9cd0-4bd9-fd1d-d10d3ab0bd81"
      },
      "source": [
        "df.loc[['blake-poems.txt','bible-kjv.txt']]"
      ],
      "id": "c221990d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>can</th>\n",
              "      <th>could</th>\n",
              "      <th>may</th>\n",
              "      <th>might</th>\n",
              "      <th>will</th>\n",
              "      <th>would</th>\n",
              "      <th>should</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>blake-poems.txt</th>\n",
              "      <td>0.476</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bible-kjv.txt</th>\n",
              "      <td>0.031</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   can  could    may  might   will  would  should\n",
              "blake-poems.txt  0.476  0.071  0.119  0.048  0.071  0.071   0.143\n",
              "bible-kjv.txt    0.031  0.024  0.149  0.069  0.552  0.064   0.111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ba3608c"
      },
      "source": [
        "- Bible kjv version is the Christian scriptures. The word 'will' has largest relative frequency among others modals. Will sometimes describe the concept of the faculty by which a person decides on and initiates action, and usually indicates intensions in Bible. \n",
        "- Poems by Blake is the poem collection of Blake, which consists numbers of poem. The word 'can' has largest relative frequency in the text. We can use the word 'can' in many ways, such as describe the permission, ability or even container. Besides, we can easy to make rhyme in the poems. \n",
        "\n",
        "*These could be the reason why the words, can and will, have largest frequency in Bible and poem respectively.*"
      ],
      "id": "2ba3608c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0fc6a58"
      },
      "source": [
        "## Inaugural"
      ],
      "id": "a0fc6a58"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff43ab2e"
      },
      "source": [
        "Choose Kennedy's speech in the Inaugural corpus"
      ],
      "id": "ff43ab2e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa2f322a",
        "outputId": "2ba53b95-1095-4d3a-ddb9-565a0b5156f2"
      },
      "source": [
        "nltk.download('inaugural')\n",
        "from nltk.corpus import inaugural"
      ],
      "id": "aa2f322a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fd249f6"
      },
      "source": [
        "text = nltk.corpus.inaugural.words('1961-Kennedy.txt')"
      ],
      "id": "5fd249f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a192cd0c"
      },
      "source": [
        "Identify the 10 most frequently use long words"
      ],
      "id": "a192cd0c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63f7f1a0",
        "outputId": "916c2802-bd9f-4346-9248-005b2bcef942"
      },
      "source": [
        "from collections import Counter\n",
        "# Find the words which have more than seven characters\n",
        "long_words = [l for l in text if len(l)>7]\n",
        "# Count the frequency of each long words\n",
        "freq_dict = {}\n",
        "for i in long_words:\n",
        "    if (i in freq_dict):\n",
        "        freq_dict[i] += 1\n",
        "    else:\n",
        "        freq_dict[i] = 1\n",
        "\n",
        "# 10 most frequently used long words\n",
        "top10 = []\n",
        "c_dict = Counter(freq_dict)\n",
        "for k, v in c_dict.most_common(10):\n",
        "    print('{}: {}'.format(k,v))\n",
        "    top10.append(k)"
      ],
      "id": "63f7f1a0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "citizens: 5\n",
            "President: 4\n",
            "Americans: 4\n",
            "generation: 3\n",
            "forebears: 2\n",
            "revolution: 2\n",
            "committed: 2\n",
            "powerful: 2\n",
            "supporting: 2\n",
            "themselves: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60394eb4"
      },
      "source": [
        "Number of synonyms of the 10 most frequently use long words"
      ],
      "id": "60394eb4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7218a76",
        "outputId": "61fe19e8-b20d-46f9-d938-7732d5715635"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "id": "e7218a76",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08711168",
        "outputId": "aeb3527c-2c0d-424b-ee5c-0ae64845512e"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "for w in top10:\n",
        "    print('Synonyms of ',w)\n",
        "    syn = []\n",
        "    for synset in wordnet.synsets(w):\n",
        "        for lemma in synset.lemmas():\n",
        "            syn.append(lemma.name())\n",
        "    print(syn)\n",
        "    print('Numbers of synonyms: {}\\n'.format(len(syn)))\n"
      ],
      "id": "08711168",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synonyms of  citizens\n",
            "['citizen']\n",
            "Numbers of synonyms: 1\n",
            "\n",
            "Synonyms of  President\n",
            "['president', 'President_of_the_United_States', 'United_States_President', 'President', 'Chief_Executive', 'president', 'president', 'chairman', 'chairwoman', 'chair', 'chairperson', 'president', 'prexy', 'President_of_the_United_States', 'President', 'Chief_Executive']\n",
            "Numbers of synonyms: 16\n",
            "\n",
            "Synonyms of  Americans\n",
            "['American', 'American_English', 'American_language', 'American', 'American']\n",
            "Numbers of synonyms: 5\n",
            "\n",
            "Synonyms of  generation\n",
            "['coevals', 'contemporaries', 'generation', 'generation', 'generation', 'generation', 'genesis', 'generation', 'generation', 'generation', 'multiplication', 'propagation']\n",
            "Numbers of synonyms: 12\n",
            "\n",
            "Synonyms of  forebears\n",
            "['forebear', 'forbear']\n",
            "Numbers of synonyms: 2\n",
            "\n",
            "Synonyms of  revolution\n",
            "['revolution', 'revolution', 'rotation', 'revolution', 'gyration']\n",
            "Numbers of synonyms: 5\n",
            "\n",
            "Synonyms of  committed\n",
            "['perpetrate', 'commit', 'pull', 'give', 'dedicate', 'consecrate', 'commit', 'devote', 'commit', 'institutionalize', 'institutionalise', 'send', 'charge', 'entrust', 'intrust', 'trust', 'confide', 'commit', 'invest', 'put', 'commit', 'place', 'commit', 'practice', 'committed', 'attached', 'committed']\n",
            "Numbers of synonyms: 27\n",
            "\n",
            "Synonyms of  powerful\n",
            "['powerful', 'knock-down', 'powerful', 'potent', 'powerful', 'brawny', 'hefty', 'muscular', 'powerful', 'sinewy', 'herculean', 'powerful', 'mighty', 'mightily', 'powerful', 'right']\n",
            "Numbers of synonyms: 16\n",
            "\n",
            "Synonyms of  supporting\n",
            "['support', 'supporting', 'support', 'back_up', 'support', 'back', 'endorse', 'indorse', 'plump_for', 'plunk_for', 'support', 'hold', 'support', 'sustain', 'hold_up', 'confirm', 'corroborate', 'sustain', 'substantiate', 'support', 'affirm', 'subscribe', 'support', 'corroborate', 'underpin', 'bear_out', 'support', 'defend', 'support', 'fend_for', 'support', 'patronize', 'patronise', 'patronage', 'support', 'keep_going', 'digest', 'endure', 'stick_out', 'stomach', 'bear', 'stand', 'tolerate', 'support', 'brook', 'abide', 'suffer', 'put_up', 'encouraging', 'supporting', 'load-bearing', 'supporting']\n",
            "Numbers of synonyms: 52\n",
            "\n",
            "Synonyms of  themselves\n",
            "[]\n",
            "Numbers of synonyms: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a587f072"
      },
      "source": [
        "**We can see the word 'supporting' in top 10 frequently used long word has largest number of synonyms with 52 synonyms.** "
      ],
      "id": "a587f072"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a58595f4"
      },
      "source": [
        "Number of hyponyms of the 10 most frequently use long words"
      ],
      "id": "a58595f4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46c5bb07",
        "outputId": "aa6bc8f9-ef12-42f0-d2bb-9538596267aa"
      },
      "source": [
        "for w in top10:\n",
        "    print('Hyponyms of',w)\n",
        "    hypo = []\n",
        "    for synset in wordnet.synsets(w):\n",
        "        for h in synset.hyponyms():\n",
        "            hypo.append(h.lemma_names())\n",
        "    print(hypo)\n",
        "    print('Numbers of hyponyms: {}\\n'.format(len(hypo)))"
      ],
      "id": "46c5bb07",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyponyms of citizens\n",
            "[['active_citizen'], ['civilian'], ['freeman', 'freewoman'], ['private_citizen'], ['repatriate'], ['thane'], ['voter', 'elector']]\n",
            "Numbers of hyponyms: 7\n",
            "\n",
            "Hyponyms of President\n",
            "[['ex-president'], ['Kalon_Tripa'], ['vice_chairman']]\n",
            "Numbers of hyponyms: 3\n",
            "\n",
            "Hyponyms of Americans\n",
            "[['African-American', 'African_American', 'Afro-American', 'Black_American'], ['Alabaman', 'Alabamian'], ['Alaskan'], ['Anglo-American'], ['Appalachian'], ['Arizonan', 'Arizonian'], ['Arkansan', 'Arkansawyer'], ['Asian_American'], ['Bay_Stater'], ['Bostonian'], ['Californian'], ['Carolinian'], ['Coloradan'], ['Connecticuter'], ['Creole'], ['Delawarean', 'Delawarian'], ['Floridian'], ['Franco-American'], ['Georgian'], ['German_American'], ['Hawaiian'], ['Idahoan'], ['Illinoisan'], ['Indianan', 'Hoosier'], ['Iowan'], ['Kansan'], ['Kentuckian', 'Bluegrass_Stater'], ['Louisianan', 'Louisianian'], ['Mainer', 'Down_Easter'], ['Marylander'], ['Michigander', 'Wolverine'], ['Minnesotan', 'Gopher'], ['Mississippian'], ['Missourian'], ['Montanan'], ['Nebraskan', 'Cornhusker'], ['Nevadan'], ['New_Englander', 'Yankee'], ['New_Hampshirite', 'Granite_Stater'], ['New_Jerseyan', 'New_Jerseyite', 'Garden_Stater'], ['New_Mexican'], ['New_Yorker'], ['Nisei'], ['North_Carolinian', 'Tarheel'], ['North_Dakotan'], ['Ohioan', 'Buckeye'], ['Oklahoman', 'Sooner'], ['Oregonian', 'Beaver'], ['Pennsylvanian', 'Keystone_Stater'], ['Puerto_Rican'], ['Rhode_Islander'], ['South_Carolinian'], ['South_Dakotan'], ['Southerner'], ['Spanish_American', 'Hispanic_American', 'Hispanic'], ['Tennessean', 'Volunteer'], ['Texan'], ['Tory'], ['Utahan'], ['Vermonter'], ['Virginian'], ['Washingtonian'], ['Washingtonian'], ['West_Virginian'], ['Wisconsinite', 'Badger'], ['Wyomingite'], ['Yankee', 'Yank', 'Northerner'], ['Yankee', 'Yank', 'Yankee-Doodle'], ['African_American_Vernacular_English', 'AAVE', 'African_American_English', 'Black_English', 'Black_English_Vernacular', 'Black_Vernacular', 'Black_Vernacular_English', 'Ebonics'], ['Creole'], ['Latin_American', 'Latino'], ['Mesoamerican'], ['North_American'], ['South_American'], ['West_Indian']]\n",
            "Numbers of hyponyms: 75\n",
            "\n",
            "Hyponyms of generation\n",
            "[['peer_group'], ['youth_culture'], ['baby_boom', 'baby-boom_generation'], ['generation_X', 'gen_X'], ['posterity'], ['biogenesis', 'biogeny']]\n",
            "Numbers of hyponyms: 6\n",
            "\n",
            "Hyponyms of forebears\n",
            "[['grandparent'], ['great_grandparent']]\n",
            "Numbers of hyponyms: 2\n",
            "\n",
            "Hyponyms of revolution\n",
            "[['Cultural_Revolution', 'Great_Proletarian_Cultural_Revolution'], ['green_revolution'], ['counterrevolution'], ['axial_rotation', 'axial_motion', 'roll'], ['dextrorotation', 'clockwise_rotation'], ['levorotation', 'counterclockwise_rotation'], ['orbital_rotation', 'orbital_motion'], ['spin']]\n",
            "Numbers of hyponyms: 8\n",
            "\n",
            "Hyponyms of committed\n",
            "[['make'], ['recommit'], ['apply'], ['rededicate'], ['vow', 'consecrate'], ['hospitalize', 'hospitalise'], ['commend'], ['consign', 'charge'], ['obligate'], ['recommit'], ['buy_into'], ['fund'], ['roll_over'], ['shelter'], ['speculate', 'job'], ['tie_up']]\n",
            "Numbers of hyponyms: 16\n",
            "\n",
            "Hyponyms of powerful\n",
            "[]\n",
            "Numbers of hyponyms: 0\n",
            "\n",
            "Hyponyms of supporting\n",
            "[['shoring', 'shoring_up', 'propping_up'], ['suspension', 'dangling', 'hanging'], ['help', 'assist', 'aid'], ['patronize', 'patronise', 'shop', 'shop_at', 'buy_at', 'frequent', 'sponsor'], ['promote', 'advance', 'boost', 'further', 'encourage'], ['second', 'back', 'endorse', 'indorse'], ['sponsor'], ['undergird'], ['fund'], ['provide', 'bring_home_the_bacon'], ['see_through'], ['sponsor', 'patronize', 'patronise'], ['subsidize', 'subsidise'], ['champion', 'defend'], ['guarantee', 'warrant'], ['block'], ['brace'], ['bracket'], ['buoy', 'buoy_up'], ['carry'], ['chock'], ['pole'], ['prop_up', 'prop', 'shore_up', 'shore'], ['scaffold'], ['truss'], ['underpin'], ['back', 'back_up'], ['document'], ['prove', 'demonstrate', 'establish', 'show', 'shew'], ['validate'], ['verify'], ['vouch'], ['apologize', 'apologise', 'excuse', 'justify', 'rationalize', 'rationalise'], ['stand_up', 'stick_up'], ['uphold'], ['accept', 'live_with', 'swallow'], ['bear_up'], ['pay'], ['sit_out'], ['stand_for', 'hold_still_for'], ['take_a_joke'], ['take_lying_down']]\n",
            "Numbers of hyponyms: 42\n",
            "\n",
            "Hyponyms of themselves\n",
            "[]\n",
            "Numbers of hyponyms: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438d7d92"
      },
      "source": [
        "**We can see the word 'Americans' in top 10 frequently used long word has largest number of hyponyms with 75 hyponyms.**"
      ],
      "id": "438d7d92"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c82e307"
      },
      "source": [
        "### Reference:\n",
        "- WordNet Interface. https://www.nltk.org/howto/wordnet.html"
      ],
      "id": "6c82e307"
    }
  ]
}
